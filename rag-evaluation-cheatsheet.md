---
title: "RAG Evaluation Cheatsheet"
description: "A compact guide to evaluating Retrieval-Augmented Generation (RAG) systems using core metrics: Groundedness, Context Relevance, and Answer Relevance."
tags: ["RAG", "LLM Evaluation", "TruLens", "Prompt Engineering", "AI Quality"]
version: v1.0
---

# ğŸ§­ **RAG Evaluation Cheatsheet**

> *Measure, diagnose, and improve Retrieval-Augmented Generation (RAG) performance with [TruLens](https://www.trulens.org/).*

---

## âš™ï¸ **The 3 Core Metrics**

| Metric | Checks | Nickname |
|--------|---------|-----------|
| **Groundedness** | Are all answers based on retrieved data? | *Hallucination Detector* |
| **Context Relevance** | Did we retrieve the *right* data? | *Retriever Health Check* |
| **Answer Relevance** | Did we answer the *actual* question? | *Response Quality Check* |

---

## ğŸ§  **1. Groundedness (Faithfulness) â€” â€œThe Hallucination Detectorâ€**

**Definition:**  
Measures whether the generated answer is *factually supported* by the retrieved context â€” no hallucinations or made-up facts.

### ğŸ§© **Example**

**Context Retrieved:**  
> â€œThe Eiffel Tower was completed in 1889 and stands 330 meters tall.â€

âœ… **Good (Grounded):**  
> â€œThe Eiffel Tower is 330 meters tall.â€

âŒ **Bad (Not Grounded):**  
> â€œThe Eiffel Tower is 330 meters tall and was designed by Gustave Eiffel in collaboration with Leonardo da Vinci.â€

**Why Bad:**  
Hallucinates information not supported by the context.

---

## ğŸ” **2. Context Relevance (Retrieval Quality) â€” â€œRight Data, Right Answerâ€**

**Definition:**  
Measures whether the retrieved documents or chunks actually contain *information relevant* to answering the userâ€™s question.

### ğŸ§© **Example**

**User Question:**  
> â€œWhat are the health benefits of green tea?â€

âœ… **Good (Relevant Context):**  
- â€œGreen tea contains antioxidants called catechins that may reduce inflammation.â€  
- â€œStudies show green tea consumption is associated with lower cardiovascular disease risk.â€

âŒ **Bad (Irrelevant Context):**  
- â€œThe tea ceremony in Japan dates back to the 9th century.â€  
- â€œGreen tea is grown primarily in China, Japan, and India.â€

**Why Bad:**  
These contexts donâ€™t help answer the question about health benefits.

---

## ğŸ’¬ **3. Answer Relevance (Response Quality) â€” â€œOn Topic or Not?â€**

**Definition:**  
Measures whether the generated answer actually *addresses the userâ€™s question* clearly and directly.

### ğŸ§© **Example**

**User Question:**  
> â€œHow do I reset my password?â€

âœ… **Good (Relevant Answer):**  
> â€œTo reset your password, click â€˜Forgot Passwordâ€™ on the login page, enter your email, and follow the link sent to your inbox.â€

âŒ **Bad (Irrelevant Answer):**  
> â€œPasswords are important for security. Strong passwords should contain uppercase, lowercase, numbers, and symbols.â€

**Why Bad:**  
Talks about passwords generally but doesnâ€™t answer *how to reset* it.

---

## ğŸ”„ **How the Three Metrics Work Together**

```mermaid
graph TD
    A[User Query] --> B[Retrieval: Context Relevance]
    B --> C[Generation: Groundedness]
    C --> D[Final Answer: Answer Relevance]
```


Each stage builds on the previous:
- **Context Relevance** â†’ Did we get the *right data*?  
- **Groundedness** â†’ Is the answer *supported by that data*?  
- **Answer Relevance** â†’ Does the response *actually answer* the question?

---

## ğŸ’¼ **End-to-End Example**

**Query:**  
> â€œWhat is the refund policy for online orders?â€

1. **Retrieval:**  
   â†’ â€œOnline orders can be returned within 30 days for a full refund.â€  
   âœ… *High Context Relevance*

2. **Generation:**  
   â†’ â€œYou can get a full refund if you return within 30 days.â€  
   âœ… *High Groundedness*

3. **Final Answer:**  
   â†’ Directly answers the refund question.  
   âœ… *High Answer Relevance*

**Result:**  
All three metrics high â†’ **High-quality RAG response!**

---

## ğŸªœ **Fix Order: Context â†’ Groundedness â†’ Answer**

> **Bad retrieval breaks everything downstream.**
> 
> Good RAG = âœ… Right Data (Context Relevance) + ğŸ§  No Hallucinations (Groundedness) + ğŸ’¬ On Topic (Answer Relevance)


---

## ğŸ“Š **Scoring Quick View**

| Metric | ğŸŸ¢ Ship It | ğŸŸ¡ Tune It | ğŸŸ  Fix It | ğŸ”´ Stop |
|--------|------------|-----------|-----------|---------|
| **Groundedness** | 0.90â€“1.0 | 0.70â€“0.89 | 0.50â€“0.69 | <0.50 |
| **Context Relevance** | 0.80â€“1.0 | 0.60â€“0.79 | 0.40â€“0.59 | <0.40 |
| **Answer Relevance** | 0.90â€“1.0 | 0.70â€“0.89 | 0.50â€“0.69 | <0.50 |

> âš–ï¸ **Note:** Thresholds should be set *case-by-case* based on your applicationâ€™s goals and tolerance for error.

---

## âš¡ï¸ **Quick Wins**

**If Groundedness is low:**  
â†’ Add to prompt: *â€œBase your answer ONLY on the provided context. If unsure, say â€˜I don't have enough information.â€™â€*

**If Context Relevance is low:**  
â†’ Improve retrieval configuration, embedding quality, or metadata filtering to return more relevant context.

**If Answer Relevance is low:**  
â†’ Add to prompt: *â€œAnswer the question directly and concisely. Stay focused on what was asked.â€*

---

## ğŸ’¡ **Rules of Thumb**

1. **You canâ€™t improve what you donâ€™t measure.**  
2. **Fix retrieval first** â€” garbage in = garbage out.  
3. **All three must be high** for reliable RAG evaluation.  
4. **Monitor continuously** â€” data and model drift can affect scores.

---

## ğŸ§© **Common Score Patterns**

| Pattern | Meaning | Fix |
|----------|----------|-----|
| All high | âœ… Ready for prod | Deploy & monitor |
| All low | ğŸ’¥ System broken | Debug fundamentals |
| Low context | ğŸ” Retrieval issue | Tune embeddings or search |
| Low groundedness | ğŸ¤¥ Hallucination | Add stricter prompts |
| Low answer | ğŸ“ Prompting | Simplify and focus |

---

## ğŸ§­ **Readiness Checklist**

- [ ] Metrics meet internal thresholds  
- [ ] Tested on diverse queries  
- [ ] Monitoring and alerts in place  
- [ ] Escalation path defined  
- [ ] Baseline metrics tracked  

---

**Evaluate your RAG system with confidence â€” measure retrieval quality, factual grounding, and response accuracy using [TruLens](https://www.trulens.org/).**


